\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{float} 
\usepackage{multirow}


%%%%%%%%%%%%%% Capsule %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\capsule}[2]{\vspace{0.5em}
  \shadowbox{%
    \begin{minipage}{.90\linewidth}%
      \textbf{#1:}~#2%
    \end{minipage}}
  \vspace{0.5em} }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcounter{ques}
\newenvironment{question}{\stepcounter{ques}{\noindent\bf Question \arabic{ques}:}}{\vspace{5mm}}

\begin{document} 

\begin{center} \Large\bf
COMP 3105 -- Assignment 3 Report\\
-- Fall 2025 -- 
\end{center} 

\begin{center}
{\bf Due:} Sunday November 16, 2025 23:59. \\
Group 51 \\
Andrew Wallace - 101210291\\
Christer Henrysson - 101260693\\[1em]
\end{center}
\textbf{Getting started} \\
Note that Python 3.11 is used for this assignment. Please install requirements using virtual environment via: \\
  \texttt{python3.11 -m venv .venv} \\
  \texttt{source .venv/bin/activate} \\
  \texttt{pip install -r requirements.txt}
\vspace{0.5em}
\newpage 

\section*{Question 1 (4\%) Linear Multi-Class Classifier}
\begin{enumerate}[(a)]
  \item (1\%) Implement a Python function \\
  \texttt{W = minMulDev(X, Y)} \\
  Recall that multinomial deviance loss is given by: \\
  \[
    W^* = \text{argmin}_W \in \mathbb{R}^{d \times k} \frac{1}{n} \sum_{i = 1}^{n} \log(1_{k}^\text{T} \exp^{W^T x_i}) - y_i^\text{T} W^\text{T} x_i
  \]  
  Please see \texttt{A3codes.py} for the implementation of \texttt{W = minMulDev(X, Y)}
  \item (1\%) Implement a Python function \\
  \texttt{Yhat = classify(Xtest, W)} \\
  Please see \texttt{A3codes.py} for the implementation of \texttt{Yhat = classify(Xtest, W)}
  \item (1\%) Implement a Python function \\
  \texttt{acc = calculateAcc(Yhat, Y)} \\
  Please see \texttt{A3codes.py} for the implementation of \texttt{acc = calculateAcc(Yhat, Y)}
  \item (1\%) In this part, you will evaluate your implementation on the synthetic datasets from above.
  The results from the synthetic classification experiment using seed 51 report the following training accuracies:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      $n$ & Model 1 & Model 2 \\
      \hline
      16 & 0.93125 & 1 \\
      32 & 0.88125 & 0.984375 \\
      64 & 0.86875 & 0.9609375\\
      128 & 0.85390625 & 0.94296875\\
      \hline
    \end{tabular}
    \caption{Training accuracies with different number of training dataset sizes}
    \label{tab:q1_classification_training}
  \end{table}
  The results from the synthetic classification experiment using seed 51 report the following test accuracies:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      $n$ & Model 1 & Model 2 \\
      \hline
      16 & 0.7317 & 0.8963\\
      32 & 0.7928& 0.8857\\
      64 & 0.8228& 0.8976\\
      128 & 0.844& 0.9178\\
      \hline
    \end{tabular}
    \caption{Test accuracies with different number of training dataset sizes}
    \label{tab:q1_classification_test}
  \end{table}
  Our results show that as the size of the input dataset increases ($n$) the accuracy decreases in both models. This is because the classifier can overfit the small dataset size. As the size of the dataset increases, there are more overlapping data points which decreases the accuracy of the classifier. We see this impacting Model 1 more than Model 2 since the data in Model 2 is more linearly separable compared to Model 1. \\[1em]
  We see the opposite effect in the test dataset. This is because the classifier becomes more generalized (less overfitting) when the training points increase. So, we see the more generalized classifier performing better to unseen datasets (the test dataset). Furthermore, the classification is less accurate on smaller dataset sizes since the classifier becomes overfitted to the training dataset. Again, we see that the accuracies are higher for Model 2 since it is more linearly separable compared to Model 1.
\end{enumerate}

\section*{Question 2 (7\%) Principle Component Analysis}
\begin{enumerate}[(a)]
  \item (1\%) Implement a Python function \\
  \texttt{U = PCA(X, k)} \\
  Please see \texttt{A3codes.py} for the implementation of \texttt{PCA(X, k)}
  \item (0.5\%) Implement a Python function \\
  \texttt{Xproj = projPCA(Xtest, mu, U)}\\
  Please see \texttt{A3codes.py} for the implementation of \texttt{projPCA(Xtest, mu, U)}
  \item (2\%) Implement a Python function \\
  \texttt{A = kernelPCA(X, k, kernel func)}\\
  Please see \texttt{A3codes.py} for the implementation of \texttt{kernelPCA(X, k, kernel func)}
  \item (2\%) Implement a Python function \\
  \texttt{Xproj = projKernelPCA(Xtest, Xtrain, kernel func, A)}\\
  Please see \texttt{A3codes.py} for the implementation of \texttt{projKernelPCA(Xtest, Xtrain, kernel func, A)}
  \item (1\%) In this part, you will evaluate your implementation on the synthetic datasets from above. Implement a Python function\\
  \texttt{train\_acc, test\_acc = synClsExperimentsPCA()}\\
  Please see \texttt{A3codes.py} for the implementation of \texttt{synClsExperimentsPCA()} \\
  The results from the synthetic PCA classification experiment using seed 51 report the following training accuracies:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      Dim $k$ & Model 1 & Model 2 \\
      \hline
      1 & 0.8503125 & 0.64242188\\
      2 & 0.85789063 & 0.94179687 \\
      \hline
    \end{tabular}
    \caption{Training accuracies for PCA classification with different dimension sizes}
    \label{tab:q2_classification_training}
  \end{table}
  The results from the synthetic PCA classification experiment using seed 51 report the following test accuracies:
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
      \hline
      Dim $k$ & Model 1 & Model 2 \\
      \hline
      1 & 0.84541 & 0.63417 \\
      2 & 0.83993 & 0.91801 \\
      \hline
    \end{tabular}
    \caption{Test accuracies for PCA classification with different dimension sizes}
    \label{tab:q2_classification_test}
  \end{table}
  \item (0.5\%) Looking at your tables from above, analyze the results and discuss any findings you may have and the possible reason behind them. \\[1em]
  We see Model 1 has similar accuracies in both the training and test data, regardless of dimension size. This is because the structure of model 1 is linear. So, when projected onto a single dimension (line), the class structure is mostly preserved and has minimal class overlap. This leads to a good accuracy when we do our prediction with multinomial deviance loss. \\[1em]
  Interestingly, we see similar accuracies when introducing a second dimension. This is because the second dimension does not have a high variance and does little in maintaining class structure in the the projection. That is, most of the variance lies in the first dimension PCA, so adding another dimension does not increase the accuracy by much. \\[1em]
  However, in Model 2 we see the accuracy increase when we introduce a second dimension (from dimension $k = 1$ to dimension $k= 2$). This is because the classes of data in Model 2 is clustered into different quadrants. So, when we project onto a single dimension, there will be overlap between the classes, leading to poor predication accuracy. When we add the second dimension, the class structure is preserved and leads to a higher prediction accuracy. We see this in both the training and test data.
\end{enumerate}

\section*{Question 3 (4\%) $k$-means}
\begin{enumerate}[(a)]
  \item (1\%) Implement a Python function \\
  \texttt{Y, U, obj val = kmeans(X, k, max iter=1000)}
  \item (1\%) Implement a Python function \\
  \texttt{Y, U, obj val = repeatKmeans(X, k, n runs=100)}
  \item (1\%) Implement a Python function \\
  \texttt{obj val list = chooseK(X, k candidates=[2,3,4,5,6,7,8,9])}
  \item (2\%) Implement a Python function \\
  \texttt{Xproj = projKernelPCA(Xtest, Xtrain, kernel func, A)}
\end{enumerate}

\section*{References}
 \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|}
      \hline
      $\lambda$ & Linear & Poly($d{=}2$) & Gauss($\sigma{=}1.0$) \\
      \hline
      0.001 & 0.958 & \textbf{0.978} & 0.493 \\
      0.01  & 0.958 & \textbf{0.978} & 0.493 \\
      0.1   & 0.958 & \textbf{0.978} & 0.493 \\
      \hline
    \end{tabular}
    \caption{Q3(c) average validation accuracies for MNIST (4 vs 9). Best setting: $\lambda{=}0.001$, Poly($d{=}2$).}
    \label{tab:q3_cv}
    \end{table}

\end{document} 
