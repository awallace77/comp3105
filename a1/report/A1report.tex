
\documentclass[12pt]{article}

\usepackage{fullpage}
\usepackage{fancybox}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{enumerate}
\usepackage{float} 


%%%%%%%%%%%%%% Capsule %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\capsule}[2]{\vspace{0.5em}
  \shadowbox{%
    \begin{minipage}{.90\linewidth}%
      \textbf{#1:}~#2%
    \end{minipage}}
  \vspace{0.5em} }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcounter{ques}
\newenvironment{question}{\stepcounter{ques}{\noindent\bf Question \arabic{ques}:}}{\vspace{5mm}}

\begin{document} 

\begin{center} \Large\bf
COMP 3105 -- Assignment 1 Report\\
-- Fall 2025 -- 
\end{center} 

\begin{center}
{\bf Due:} Sunday September 28, 2025 23:59. \\
Group 51 \\
Andrew Wallace - 101210291\\
Christer Henrysson - 101260693\\[1em]
\end{center}
\textbf{Getting started} \\
Note that Python 3.11 was used for this assignment. Please install requirements using virtual environment via: \\
  \texttt{python3.11 -m venv .venv} \\
  \texttt{source .venv/bin/activate} \\
  \texttt{pip install -r requirements.txt}
\vspace{0.5em}
\newpage 

\begin{question} \textbf{(7.5\%) Linear Regression}

  \begin{enumerate}[(a)] 
    \item \textbf{(1\%) $L_2$ Regression} \\
    Please see A1codes.py for implementation.
    \item (3\%) \textbf{$L_\infty$ Regression} \\
      Here we are going to solve the $L_\infty$ loss regression problem
      \begin{center}
        $\mathbf{w} = \text{argmin}_{\mathbf{w} \in \mathbb{R}^d} = ||X \mathbf{w} - \mathbf{y}||$
      \end{center}
      Recall that this optimization can be expressed as a linear programming problem with the joint paramters $\begin{bmatrix}
        \mathbf{w} \\[-0.5em] 
        \substack{d \times 1} \\[0.15em]
        \delta \\[-0.5em] 
        \substack{1 \times 1} \\[0.15em]
      \end{bmatrix} \in \mathbb{R}^{d+1}$ as follows
      \begin{center}
        $\text{min}_{\mathbf{w}, \delta} \delta$ \\
        s.t. \\
        $\delta \ge 0 \iff -\delta \le 0$ \\
        $X \mathbf{w} - \mathbf{y} \preceq \delta \cdot \mathbf{1}_n \iff X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq \mathbf{y}$ \\
        $\mathbf{y} - X \mathbf{w} \preceq \delta \cdot \mathbf{1}_n \iff -X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq -\mathbf{y}$
      \end{center}
      In the following answers, we will convert the optimization to a form that is solvable by the \textbf{cxvopt} linear programming (LP) solver, which solves the following form of LP
      \begin{center}
        $\text{min}_{\mathbf{u}}\mathbf{c}^T \mathbf{u}$ \\
        s.t. $G \mathbf{u} \preceq \mathbf{h}$
      \end{center}
      Let the unknown variables be $\mathbf{u} = \begin{bmatrix}
        \mathbf{w} \\[-0.5em] 
        \substack{d \times 1} \\[0.15em]
        \delta \\[-0.5em] 
        \substack{1 \times 1} \\[0.15em]
      \end{bmatrix} \in \mathbb{R}^{d+1}$ \\
      For the constraints, since we have three sets of constraints, the matrix $G$ and $\mathbf{h}$ can be decomposed into three parts 
      \begin{center}
        $G \cdot \mathbf{u} = 
        \begin{bmatrix}
          G^{(1)} \\[-0.5em] 
          \substack{1 \times (d+1)} \\[0.15em]
          G^{(2)} \\[-0.5em] 
          \substack{n \times (d+1)} \\[0.15em]
          G^{(3)} \\[-0.5em] 
          \substack{n \times (d+1)} \\[0.15em]
        \end{bmatrix} \cdot 
        \begin{bmatrix}
          \mathbf{w} \\
          \delta 
        \end{bmatrix} \preceq 
        \begin{bmatrix}
          \mathbf{h}^{(1)} \\[-0.5em] 
          \substack{1 \times 1} \\[0.15em]
          \mathbf{h}^{(2)} \\[-0.5em] 
          \substack{d \times 1} \\[0.15em]
          \mathbf{h}^{(3)} \\[-0.5em] 
          \substack{d \times 1} \\[0.15em]
        \end{bmatrix}$
      \end{center}
    \begin{enumerate}[(b.1)]
      \item (0.25\%) For the objective function, we want $\mathbf{c}^T \mathbf{u} = \delta$. What should $\mathbf{c} \in \mathbb{R}^{d+1}$ be? 
      Recall that $\mathbf{u} = \begin{bmatrix} \mathbf{w} \\ \delta \end{bmatrix}$ \\
      For the objective function to be $\delta$ we have: \\
      $\mathbf{c}^T \mathbf{u} = [c_1, c_2, \ldots c_d, c_{d+1}] \cdot 
      \begin{bmatrix}
        w_1 \\
        w_2 \\
        \vdots \\
        w_d \\
        \delta 
        \end{bmatrix} = c_1 w_1 + c_2 w_2 + \ldots + w_d u_d + c_{d+1} \delta$ \\
      Now let $c_1 = c_2 = \ldots = c_d = 0$ \\
      And $c_{d+1} = 1$ \\
      This gives us: \\
      $0w_1 + 0w_2 + \ldots 0w_d + \delta$ \\
      $=\delta$. \\
      Thus $\mathbf{c} = \begin{bmatrix}
        0 \\
        0 \\
        \vdots \\
        0 \\
        1
        \end{bmatrix}$ \\
      Where $c_1 = c_2 = \ldots = c_d =0$ and $c_{d+1} = 1$.
      \item (0.25\%) We want $G^{(1)}\mathbf{u} \preceq \mathbf{h}^{(1)} \iff \delta \ge 0.$ What should $G^{(1)} \in \mathbb{R}^{1 \times (d+1)}$ and $h^{(1)} \in \mathbb{R}$ be? \\
      Recall the first constraint: $-\delta \le 0$ \\
      We want
      \begin{center}
        $\underset{1 \times (d + 1)}{G^{(1)}} \cdot \mathbf{u} \preceq \underset{1 \times 1}{\mathbf{h}^{(1)}}$ \\
      \end{center}
      We will now map our constraint into this form.
      \begin{center}
        $\begin{bmatrix}
          \underset{1 \times d}{G^{(11)}} && \underset{1 \times 1}{G^{(12)}} 
        \end{bmatrix} \cdot 
        \begin{bmatrix}
          \mathbf{w} \\
          \delta
        \end{bmatrix} \preceq 
        \underset{n \times 1}{\mathbf{h}^{(1)}} \iff -\delta \le 0$ \\
        $\underset{1 \times d}{G^{(11)}} \cdot \mathbf{w} + \underset{1 \times 1}{G^{(12)}} \cdot \delta = -\delta \le 0 = \underset{1 \times 1}{\mathbf{h}^{(1)}}$ \\
      \end{center}
      So, \\
      $\underset{1 \times d}{G^{(11)}} = \mathbf{0}_{1 \times d}$ \\
      $\underset{1 \times 1}{G^{(12)}} = -1$ \\
      $\underset{1 \times (d+1)}{G^{(1)}} = 
      \begin{bmatrix}
        \mathbf{0}_{1 \times d} & -1 
      \end{bmatrix}$ \\
      $\underset{1 \times 1}{\mathbf{h}^{(1)}} = \mathbf{0}_{1 \times 1}$
      \item (0.25\%) We want $G^{(2)}\mathbf{u} \preceq \mathbf{h}^{(2)} \iff X \mathbf{w} - \mathbf{y} \preceq \delta \cdot \mathbf{1_n}.$ What should $G^{(2)} \in \mathbb{R}^{n \times (d+1)}$ and $h^{(2)} \in \mathbb{R}^n$ be? \\
      Recall our second constraint is \\
      $X \mathbf{w} - \mathbf{y} \preceq \delta \cdot \mathbf{1}_n \iff X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq \mathbf{y}$ \\
      We want
      \begin{center}
        $\underset{n \times (d + 1)}{G^{(2)}} \cdot \mathbf{u} \preceq \underset{n \times 1}{\mathbf{h}^{(2)}}$ \\
      \end{center}
      We will now map our constraint into this form.
      \begin{center}
        $\begin{bmatrix}
          \underset{n \times d}{G^{(21)}} && \underset{n \times 1}{G^{(22)}} 
        \end{bmatrix} \cdot 
        \begin{bmatrix}
          \mathbf{w} \\
          \delta
        \end{bmatrix} \preceq 
        \underset{n \times 1}{\mathbf{h}^{(2)}} \iff X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq \mathbf{y}$ \\
        $\underset{n \times d}{G^{(21)}} \cdot \mathbf{w} + \underset{n \times 1}{G^{(22)}} \cdot \delta = X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq \mathbf{y} = \underset{n \times 1}{\mathbf{h}^{(1)}}$ \\
      \end{center}
      So, \\
      $\underset{n \times d}{G^{(21)}} = X$ \\
      $\underset{n \times 1}{G^{(22)}} = -\mathbf{1}_{n \times 1}$ \\
      $\underset{n \times (d+1)}{G^{(2)}} = 
      \begin{bmatrix}
        X & -\mathbf{1}_{n \times 1}
      \end{bmatrix}$ \\
      $\underset{n \times 1}{h^{(2)}} = \mathbf{y}$
      \item (0.25\%) We want $G^{(3)}\mathbf{u} \preceq \mathbf{h}^{(3)} \iff \mathbf{y} - X \mathbf{w} \preceq \delta \cdot \mathbf{1_n}.$ What should $G^{(3)} \in \mathbb{R}^{n \times (d+1)}$ and $h^{(3)} \in \mathbb{R}^n$ be?\\
      Recall our third constraint is \\
      $\mathbf{y} - X \mathbf{w} \preceq \delta \cdot \mathbf{1}_n \iff -X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq -\mathbf{y}$ \\
      We want
      \begin{center}
        $\underset{n \times (d + 1)}{G^{(3)}} \cdot \mathbf{u} \preceq \underset{n \times 1}{\mathbf{h}^{(3)}}$ \\
      \end{center}
      We will now map our constraint into this form.
      \begin{center}
        $\begin{bmatrix}
          \underset{n \times d}{G^{(31)}} && \underset{n \times 1}{G^{(32)}} 
        \end{bmatrix} \cdot 
        \begin{bmatrix}
          \mathbf{w} \\
          \delta
        \end{bmatrix} \preceq 
        \underset{n \times 1}{\mathbf{h}^{(3)}} \iff -X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq -\mathbf{y}$ \\
        $\underset{n \times d}{G^{(31)}} \cdot \mathbf{w} + \underset{n \times 1}{G^{(32)}} \cdot \delta = -X \mathbf{w} - \delta \cdot \mathbf{1}_n \preceq -\mathbf{y} = \underset{n \times 1}{\mathbf{h}^{(3)}}$ \\
      \end{center}
      So, \\
      $\underset{n \times d}{G^{(31)}} = -X$ \\
      $\underset{n \times 1}{G^{(32)}} = -\mathbf{1}_{n \times 1}$ \\
      $\underset{n \times (d+1)}{G^{(3)}} = 
      \begin{bmatrix}
        -X & -\mathbf{1}_{n \times 1}
      \end{bmatrix}$ \\
      $\underset{n \times 1}{h^{(3)}} = -\mathbf{y}$
      
      \item (2\%) Based on your derivations in (b), implement a Python function \begin{center}$\mathbf{w} = \text{minimizeLinf}(X,y)$ \end{center} that returns a $d \times 1$ vector of weights/parameters $\mathbf{w}$ corresponding to the solution of minimum $L_\infty$ loss.
      \begin{center}
        $\mathbf{w} = \text{argmin}_{w \in \mathbb{R}^d} || X \mathbf{w} - \mathbf{y}||$
      \end{center}

      Based on our derivations in (b), we result in \\
      $\mathbf{c} = \begin{bmatrix}
        0 \\
        0 \\ 
        \vdots \\
        1
      \end{bmatrix} \in \mathbb{R}^{d+1}$ \\[0.5em]
      $\mathbf{u} = \begin{bmatrix}
        \mathbf{w} \\
        \delta
      \end{bmatrix} \in \mathbb{R}^{d+1}$ \\[0.5em]
      $G = \begin{bmatrix}
        \mathbf{0}_{1 \times d} & -1 \\
        X & -\mathbf{1}_n \\
        -X & -\mathbf{1}_n
      \end{bmatrix} \in \mathbb{R}^{(2n+1) \times (d+1)}$ \\[0.5em]
      $\mathbf{h} = \begin{bmatrix}
        0 \\
        \mathbf{y} \\
        -\mathbf{y}
      \end{bmatrix} \in \mathbb{R}^{2n+1}$ 
      \begin{center}
        s.t. \\
        $\underset{(2n+1) \times (d+1)}{G} \cdot \underset{(d+1) \times 1}{\mathbf{u}} \preceq \underset{(2n+1) \times 1}{\mathbf{h}}$
      \end{center}


      Please see A1codes.py for full implementation. 
    \end{enumerate}

    \item \textbf{(2\%) Synthetic Regression Problem}  \\
    In this part, you will evaluate your implemented algorithms on a synthetic dataset.
    \begin{enumerate}[(c.1)]
      \item (1\%) Implement a Python function \\
      \texttt{train\_loss, test\_loss = synRegExperiments()} \\that returns a 2 x 2 matrix train loss of average training losses and a 2 x 2 matrix test loss of average test losses (See Table 1 and Table 2 below.) It repeats 100 runs as follows \\
      Please see \textbf{A1codes.py} for full details.
      \item (1\%) Looking at your tables from above, analyze the results and discuss any findings you may have and the possible reason behind them. \\
      Note that the matrices below are representative of the following table:
      \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          Model & Average $L_2$ loss & Average $L_\infty$ loss \\ 
          \hline
          $L_2$ model &  &  \\ 
          $L_\infty$ model &  &  \\ 
          \hline
        \end{tabular}
        \caption{Table representation of loss data}
        \label{tab:basic_table}
      \end{table}
      After running \texttt{synRegExperiments} on the regression\_train.csv and regression\_test.csv data sets we get the following results: \\
      For training data:
      \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          Model & Average $L_2$ loss & Average $L_\infty$ loss \\ 
          \hline
          $L_2$ model & 2.00305568 & 7.31711154 \\ 
          $L_\infty$ model & 2.36347873 & 6.6215357 \\ 
          \hline
        \end{tabular}
        \caption{Training losses for $L_2$ and $L_\infty$ models on regression\_train.csv}
        \label{tab:training_losses_csv}
      \end{table}
      For test data: 
      \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          Model & Average $L_2$ loss & Average $L_\infty$ loss \\ 
          \hline
          $L_2$ model & 1.66071948 & 5.58874693 \\ 
          $L_\infty$ model & 1.9868478 & 5.02664782 \\ 
          \hline
        \end{tabular}
        \caption{Test losses for $L_2$ and $L_\infty$ models on regression\_test.csv}
        \label{tab:test_losses_csv}
      \end{table}
      For a randomly generated training and test data set using seed 101210291, we get the following results: \\[0.5 em]
      For training data:
      \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          Model & Average $L_2$ loss & Average $L_\infty$ loss \\ 
          \hline
          $L_2$ model & 0.11160734 & 1.68718399 \\ 
          $L_\infty$ model & 0.27832791 & 0.91119205 \\ 
          \hline
        \end{tabular}
        \caption{Training losses for $L_2$ and $L_\infty$ models on random data seeded with 101210291}
        \label{tab:train_losses_rand}
      \end{table}
      For test data:
      \begin{table}[H]
        \centering
        \begin{tabular}{|c|c|c|}
          \hline
          Model & Average $L_2$ loss & Average $L_\infty$ loss \\ 
          \hline
          $L_2$ model & 0.05252988 & 1.02181983 \\ 
          $L_\infty$ model & 0.35054548 & 2.26890905 \\ 
          \hline
        \end{tabular}
        \caption{Test losses for $L_2$ and $L_\infty$ models on random data seeded with 101210291}
        \label{tab:test_losses_rand}
      \end{table}
      This shows that our $L_2$ loss, overall, has a lower average loss compared to the $L_\infty$ loss. This suggests that the $L_2$ model fits the data better and will perform better compared to the $L_\infty$ model for this population. It is shown with both the training and test data that the $L_2$ model has a lower average $L_2$ loss than the $L_\infty$ loss. However, we see that in the $L_2$ model, the $L_\infty$ loss is significantly worse than the $L_\infty$ model, suggesting that there are a few individual points with high error.
    \end{enumerate}
    \item \textbf{(1.5\%) Real-World Regression Problem}
      \begin{enumerate}[(d.1)]
        \item (1\%) Here you will apply the linear regression algorithm to the concrete compressive strength (CCS) dataset. \\
        Please see \textbf{A1codes.py} for \texttt{preprocessCCS(dataset\_folder)} implemenation.
        \item (0.5 \%) Implement a Python function \\
        Please see \textbf{A1codes.py} for \texttt{runCCS(dataset\_folder)} implemenation. \\
        For training data:
        \begin{table}[H]
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            Model & Average $L_2$ loss & Average $L_\infty$ loss \\ 
            \hline
            $L_2$ model & 52.7794 & 32.6402 \\ 
            $L_\infty$ model & 65.5936 & 26.2982 \\ 
            \hline
          \end{tabular}
          \caption{Training losses for $L_2$ and $L_\infty$ models on CCS dataset}
          \label{tab:ccs_train}
        \end{table}

        For test data:
        \begin{table}[H]
          \centering
          \begin{tabular}{|c|c|c|}
            \hline
            Model & Average $L_2$ loss & Average $L_\infty$ loss \\ 
            \hline
            $L_2$ model & 55.6988 & 33.3930 \\ 
            $L_\infty$ model & 68.1230 & 37.3159 \\ 
            \hline
          \end{tabular}
          \caption{Test losses for $L_2$ and $L_\infty$ models on CCS dataset}
          \label{tab:ccs_test}
        \end{table}

      \end{enumerate}
  \end{enumerate}
\end{question}
\begin{question} \textbf{(7.5\%) Logistic Regression}\\
In this question, you will implement logistic regression, a classification method, and
solve it using scipy.optimize.minimize.
All of the following functions must be able to handle arbitrary $n > 0$ and $d > 0$.
The vectors and matrices are represented as numpy arrays. Your functions shouldn't
print additional information to the standard output. \\
\begin{enumerate}[(a)]
  \item \textbf{(2\% Solving Convex Problem)}
  \begin{enumerate}[(a.1)]
    \item (1\%) Before diving into logistic regression, let's first revisit the linear regression in Q1(a). Implement two python functions \\
    Please see \textbf{A1codes.py} for the implemenation of \texttt{linearRegL2Obj(w, X, y)} and \texttt{linearRegL2Grad(w, X, y)}.
    \item (1\%) Write a python function \\
    Please see \textbf{A1codes.py} for the implemenation of \texttt{find\_opt(obj\_func, grad\_func, X, y)}.
  \end{enumerate}
  \item \textbf{(2\% Solving Logistic Regression)} \\
  Implement two Python functions \\
  Please see \textbf{A1codes.py} for the implemenation of \texttt{logisticRegObj(w, X, y)} and \texttt{logisticRegGrad(w, X, y)}.
  \item \textbf{(2\% Synthetic Binary classification Problem)}
  \begin{enumerate} [(c.1)]
    \item (1\%) In this part, you will evaluate your implemenation on a synthetic dataset. Implement a Python function \\
    Please see \textbf{A1codes.py} for the implemenation of \texttt{synClsExperiments()}. \\[0.5em]
    After running \texttt{synClsExperiments} on the classification\_train.csv and classification\_test.csv data sets we get the following results:
    \begin{enumerate}[$\cdot$]
      \item For training data we get an accuracy of $0.93$
      \item For test data we get an accuracy of $0.925$
    \end{enumerate}
    For a randomly generated training and test data set using seed 101210291, we get the following results: \\[0.5 em]
    For training data:
    \begin{table}[H]
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        \texttt{m} Train Accuracy & \texttt{dim1} Train Accuracy & \texttt{dim2} Train Accuracy \\ 
        \hline
        $10 = 0.9795 $ & $1 = 0.8421 $ & $1 = 0.92535 $ \\ 
        $50 = 0.923 $ & $2 = 0.93095 $ & $2 = 0.92585$ \\ 
        $100 = 0.92565 $ & $4 = 0.989 $ & $4 = 0.92755$ \\ 
        $200 = 0.925225 $ & $8 = 1. $ & $8 = 0.93285$ \\ 
        \hline
      \end{tabular}
      \caption{Training accuracy for different hyper-parameters for random data seeded with 101210291}
      \label{tab:train_losses_cls_csv}
    \end{table}

    For test data:
    \begin{table}[H]
      \centering
      \begin{tabular}{|c|c|c|}
        \hline
        \texttt{m} Test Accuracy & \texttt{dim1} Test Accuracy & \texttt{dim2} Test Accuracy \\ 
        \hline
        $10 = 0.875145 $ & $1 = 0.83864 $ & $1 = 0.918075$ \\ 
        $50 = 0.9131$ & $2 = 0.91621 $ & $2 = 0.916885$ \\ 
        $100 = 0.916855$ & $4 = 0.969865 $ & $4 = 0.914395$ \\ 
        $200 = 0.919775$ & $8 = 0.99356$ & $8 = 0.90897$ \\ 
        \hline
      \end{tabular}
      \caption{Test accuracy for different hyper-parameters for random data seeded with 101210291}
      \label{tab:test_losses_cls_csv}
    \end{table}
    \item (1\%) Looking at your tables from above, analyze the results and discuss any findings you may have and the possible reasons behind them \\
    The initial testing done on the real world data from the csv files shows that our logistic regression model is performing well with minimal accuracy drop when seeing new (test) data. \\
    In our randomized dataset (seeded with 101210291), we see that as we introduce more training data (\texttt{m}), the training and test accuracies both improve. Initially, the training accuracy is high ($\approx97\%$) for \texttt{m} = 10, but is lower for test accuracy ($\approx 87\%$). This suggests that the model is overfitting for a small amount of training data. The results show that the as we train on more data, the model becomes more stable, and test accuracy improves. Furthermore, as we increase the number of features, specified in \texttt{dim1}, we see that both training and test accuracies converge to 1. This shows that the more classification data we can train on, the better our model will be at predicting new data. Lastly, we see that in \texttt{dim2}, the accuracies stay more steady as the number of features increase. That is becuase these features are not weighted (-1/+1) like we do in \texttt{dim1} demonstrating that features that don't provide valuable information don't necessarily increase our accuracy. 
  \end{enumerate}
  \item \textbf{(1.5\%) Real-World Binary Classification Problem}
  \begin{enumerate}[(d.1)]
    \item (1\%) Now you will apply logistic regression a real-world problem, the Breast Cancer Wisconsin (BCW) dataset\\
    To start, you need to preprocess the data. Implement a Python function \\
    Please see \textbf{A1codes.py} for \texttt{preprocessBCW(dataset\_folder)} implemenation.
    \item (0.5\%) Implement a Python function\\
    Please see \textbf{A1codes.py} for \texttt{runBCW(dataset\_folder)} implemenation.

  \end{enumerate}
\end{enumerate}

\end{question}

\end{document} 
